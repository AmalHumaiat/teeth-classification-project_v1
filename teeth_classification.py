# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j9TNH2guTb-KPq0LzecW1u41a1fpn2Ps

# ***Libraries***
"""

import numpy as np
import pandas as pd
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

"""# ***Load data***"""

import os
dataset_path="/content/drive/MyDrive/Teeth_Dataset"
train_ds=tf.keras.preprocessing.image_dataset_from_directory(
    f"{dataset_path}/Training", image_size=(256,256),batch_size=32
)
valid_ds=tf.keras.preprocessing.image_dataset_from_directory(
    f"{dataset_path}/Validation", image_size=(256,256),batch_size=32
)
test_ds=tf.keras.preprocessing.image_dataset_from_directory(
    f"{dataset_path}/Testing", image_size=(256,256),batch_size=32
)

print("Class names:", train_ds.class_names)

"""# ***Data Visualization***"""

class_names = train_ds.class_names
num_classes = len(class_names)

# Count images per class
counts = {cls: 0 for cls in class_names}
for images, labels in train_ds:
    for label in labels.numpy():
        counts[class_names[label]] += 1

# Plot distribution
plt.figure(figsize=(8, 6))
sns.barplot(x=list(counts.keys()), y=list(counts.values()))
plt.xticks(rotation=45)
plt.xlabel("Class")
plt.ylabel("Number of Images")
plt.title("Class Distribution in Training Data")
plt.show()

plt.figure(figsize=(10, 5))
for images, labels in train_ds.take(1):
    for i in range(9):
        plt.subplot(3, 3, i+1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
plt.show()

"""# ***Data Augmentation***"""

data_augmentation=tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
    tf.keras.layers.RandomBrightness(0.2),
    tf.keras.layers.RandomContrast(0.2)
])

plt.figure(figsize=(10, 5))
for images, _ in train_ds.take(1):
    for i in range(9):
        augmented_img = data_augmentation(images)
        plt.subplot(3, 3, i+1)
        plt.imshow(augmented_img[i].numpy().astype("uint8"))
        plt.axis("off")
plt.show()

"""# ***Preprocessing***"""

normalization_layer = tf.keras.layers.Rescaling(1./255)
normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
normalized_valid_ds = valid_ds.map(lambda x, y: (normalization_layer(x), y))
normalized_test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))

"""# ***Model Architecture***"""

from tensorflow.keras import layers, models
model=models.Sequential([
    data_augmentation,
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')

])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    normalized_train_ds,
    validation_data=normalized_valid_ds,
    epochs=50
)
loss, accuracy = model.evaluate(normalized_test_ds)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

model = models.Sequential([
    data_augmentation,
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(),

    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(),

    layers.Conv2D(256, (3,3), activation='relu', padding='same'),
    layers.Conv2D(256, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    normalized_train_ds,
    validation_data=normalized_valid_ds,
    epochs=50
)
loss, accuracy = model.evaluate(normalized_test_ds)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""# ***Pretrained model ResNet50***"""

from tensorflow.keras.applications import ResNet50

# Load the pretrained ResNet50 model, excluding the fully connected layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))

# Freeze the base model's layers (fine-tuning comes later)
base_model.trainable = False

# Build the model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(normalized_train_ds, epochs=50, validation_data=normalized_valid_ds)

# Fine-tune by unfreezing some layers
base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Continue training
model.fit(normalized_train_ds, epochs=50, validation_data=normalized_valid_ds)

loss, accuracy = model.evaluate(normalized_test_ds)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""# **Reeduce Overfitting with Regularization**"""

from tensorflow.keras import regularizers

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(normalized_train_ds, epochs=50, validation_data=normalized_valid_ds)

# Fine-tune by unfreezing some layers
base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Continue training
model.fit(normalized_train_ds, epochs=32, validation_data=normalized_valid_ds)

loss, accuracy = model.evaluate(normalized_test_ds)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""# ***Saving***"""

model.save("teeth_classification_model.h5")

